{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Mental Health Chatbot with Fine-tuned Falcon-7B\n\nThis project implements a Mental Health Chatbot using Transformers, PEFT (Parameter-Efficient Fine-Tuning). The chatbot uses a fine-tuned Falcon-7B model to generate responses to user queries about mental health topics.\n\n## Installation\nTo run this chatbot, make sure to install the required dependencies. Use the following commands to install them:\n\n```sh\n# Install core dependencies\n!pip install trl==0.6.0 transformers accelerate peft gradio -Uqqq\n\n# Install additional dependencies for model optimization\n!pip install datasets bitsandbytes einops wandb -Uqqq\n```\n\nThese packages include:\n\n- `transformers:` For working with pre-trained language models.\n- `accelerate:` For efficient multi-GPU and mixed-precision training.\n- `peft:` For parameter-efficient fine-tuning.\n- `gradio:` For building the chatbot interface.\n- `datasets:` For loading and processing datasets.\n- `bitsandbytes:` For 4-bit quantization of the model.\n- `einops:` For tensor manipulation.\n- `wandb:` For logging and monitoring model training.\n\n## Code Modules\nThe implementation is structured into multiple functions to maintain modularity and readability.\n\n### 1. Model and Tokenizer Initialization\nThis function initializes the model and tokenizer using the Falcon-7B model with PEFT and bitsandbytes for efficient model loading and inference.\n\n### 2. Fine-Tuning Model with PEFT\nThis function prepares the model for PEFT (Low-Rank Adaptation) fine-tuning using LoraConfig.\n\n### 3. Training the Model\nThis section includes the setup for model training with SFTTrainer.\n\n### 4. Generating Responses\nThis function generates responses from both the original model and the fine-tuned PEFT model.\n","metadata":{}},{"cell_type":"code","source":"\"\"\"\nFalcon-7b-Model-Finetuned-By-Mental-Health-Data\n=====================================\nThis project implements a Mental Health Chatbot using Transformers, PEFT (Parameter-Efficient Fine-Tuning), and Gradio. \nThe chatbot uses a fine-tuned Falcon-7B model to generate responses to user queries about mental health topics.\n\"\"\"\n\n# Import necessary libraries and modules\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, GenerationConfig\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:08:06.320830Z","iopub.execute_input":"2025-02-25T17:08:06.321132Z","iopub.status.idle":"2025-02-25T17:08:28.602582Z","shell.execute_reply.started":"2025-02-25T17:08:06.321107Z","shell.execute_reply":"2025-02-25T17:08:28.601946Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================\n# Login to Hugging Face Hub\n# ============================\n\nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:08:33.973688Z","iopub.execute_input":"2025-02-25T17:08:33.973985Z","iopub.status.idle":"2025-02-25T17:08:33.997961Z","shell.execute_reply.started":"2025-02-25T17:08:33.973964Z","shell.execute_reply":"2025-02-25T17:08:33.996954Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3824c125a7794fac959779b9165260b1"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ============================\n# Load the mental health chatbot dataset\n# ============================\n\ndata = load_dataset(\"heliosbrahma/mental_health_chatbot_dataset\")\nprint(data[\"train\"][0]['text']) # Print a sample from the dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:08:57.997993Z","iopub.execute_input":"2025-02-25T17:08:57.998285Z","iopub.status.idle":"2025-02-25T17:08:59.985919Z","shell.execute_reply.started":"2025-02-25T17:08:57.998262Z","shell.execute_reply":"2025-02-25T17:08:59.985199Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01384fd663764d65900309cf34b57d43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-01391a60ef5c00d9.parquet:   0%|          | 0.00/102k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a7bbd1ff6f44d53b83f1cb64d2393ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/172 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c789345c5e14df2be013713c2e2bccf"}},"metadata":{}},{"name":"stdout","text":"<HUMAN>: What is a panic attack?\n<ASSISTANT>: Panic attacks come on suddenly and involve intense and often overwhelming fear. They’re accompanied by very challenging physical symptoms, like a racing heartbeat, shortness of breath, or nausea. Unexpected panic attacks occur without an obvious cause. Expected panic attacks are cued by external stressors, like phobias. Panic attacks can happen to anyone, but having more than one may be a sign of panic disorder, a mental health condition characterized by sudden and repeated panic attacks.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================\n# Define model and tokenizer\n# ============================\n\n\nmodel_name = \"ybelkada/falcon-7b-sharded-bf16\" # sharded falcon-7b model\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, # Use bitsandbytes config\n    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n    trust_remote_code=True, # Set trust_remote_code=True to use falcon-7b model with custom code\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:09:06.686143Z","iopub.execute_input":"2025-02-25T17:09:06.686427Z","iopub.status.idle":"2025-02-25T17:10:42.424648Z","shell.execute_reply.started":"2025-02-25T17:09:06.686404Z","shell.execute_reply":"2025-02-25T17:10:42.423983Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/581 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e91d9c3e84ea4c48985bb0103c7736d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dbeacfb66af475ca0b2727ce88d7edc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe57c44d27434ef8ac3d2846ba34fc00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/1.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34215cb330e447a7bad0cd139d626b3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b9e6045fc024db3a8c33b2eaa38efe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d82b9691bc6e4049a2143e84f955baa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0b5a9544a754687acf17dfe5e9730d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a62f4a5f9b14adc9e479134c0fe1cc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82756b9a3908466aab7905103abc82b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"855ebf12a2194ee5b3524cb0d999123e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/921M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcdcfdda870b4cf4a2d2ddf227d5dd0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28171f243e914f599a4c33e3e8bdeb35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f663300ecf4a74bf5e1de853f372f5"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ============================\n# Load tokenizer for the model\n# ============================\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) # Set trust_remote_code=True\ntokenizer.pad_token = tokenizer.eos_token # Setting pad_token same as eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:10:48.449176Z","iopub.execute_input":"2025-02-25T17:10:48.449463Z","iopub.status.idle":"2025-02-25T17:10:50.212090Z","shell.execute_reply.started":"2025-02-25T17:10:48.449442Z","shell.execute_reply":"2025-02-25T17:10:50.211114Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b89db7812fd9486985becc275441aa53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e30b07863a473fa07ffb2ee7669687"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac8d41a30de5437eba30e5373ed7829b"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# ============================\n# Prepare model for k-bit training\n# ============================\n\n\nmodel = prepare_model_for_kbit_training(model)\n\n# Define LoRA configurations for efficient training\nlora_alpha = 32 # scaling factor for the weight matrices\nlora_dropout = 0.05 # dropout probability of the LoRA layers\nlora_rank = 32 # dimension of the low-rank matrices\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\",\n    target_modules=[  # Setting names of modules in falcon-7b model that we want to apply LoRA to\n        \"query_key_value\",\n        \"dense\",\n        \"dense_h_to_4h\",\n        \"dense_4h_to_h\",\n    ]\n)\n\n# Apply LoRA to the model\npeft_model = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:10:56.807223Z","iopub.execute_input":"2025-02-25T17:10:56.807597Z","iopub.status.idle":"2025-02-25T17:10:57.767617Z","shell.execute_reply.started":"2025-02-25T17:10:56.807526Z","shell.execute_reply":"2025-02-25T17:10:57.766942Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ============================\n# Training configurations\n# ============================\n\n\noutput_dir = \"./Falcon-7b-Model-Finetuned-By-Mental-Health-Data\"\nper_device_train_batch_size = 4 # reduce batch size by 2x if out-of-memory error\ngradient_accumulation_steps = 16  # increase gradient accumulation steps by 2x if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 1 # number of updates steps before two checkpoint saves\nlogging_steps = 1  # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = 10        # training will happen for 320 steps\nwarmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\n\n# Define training arguments for the fine-tuning process\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    bf16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    push_to_hub=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:11:37.784010Z","iopub.execute_input":"2025-02-25T17:11:37.784333Z","iopub.status.idle":"2025-02-25T17:11:37.818936Z","shell.execute_reply.started":"2025-02-25T17:11:37.784303Z","shell.execute_reply":"2025-02-25T17:11:37.818055Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ============================\n# Tokenization function for preparing dataset\n# ============================\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024\n    )\n\n# Tokenizing the dataset\ntokenized_dataset = data.map(tokenize_function, batched=True)\ntokenized_dataset = tokenized_dataset.remove_columns([\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:12:03.134741Z","iopub.execute_input":"2025-02-25T17:12:03.135112Z","iopub.status.idle":"2025-02-25T17:12:03.377057Z","shell.execute_reply.started":"2025-02-25T17:12:03.135083Z","shell.execute_reply":"2025-02-25T17:12:03.376181Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/172 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03ce66a8f00d4d9b890276c7ee978ae0"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# ============================\n# Initialize the trainer for fine-tuning\n# ============================\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=tokenized_dataset[\"train\"],  #Use preprocessed dataset\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:12:08.718500Z","iopub.execute_input":"2025-02-25T17:12:08.718820Z","iopub.status.idle":"2025-02-25T17:12:09.323335Z","shell.execute_reply.started":"2025-02-25T17:12:08.718788Z","shell.execute_reply":"2025-02-25T17:12:09.322499Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/172 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f60e6e0ad94acba632235319a29e10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/172 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"419b457b8f3e415293b881d10bb60502"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/172 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed45f492916549a08982068c6ca59e2d"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================\n# Upcasting the layer norms in torch.bfloat16 for more stable training\n# ============================\n\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.bfloat16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:12:13.864368Z","iopub.execute_input":"2025-02-25T17:12:13.864732Z","iopub.status.idle":"2025-02-25T17:12:13.874599Z","shell.execute_reply.started":"2025-02-25T17:12:13.864699Z","shell.execute_reply":"2025-02-25T17:12:13.873640Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ============================\n# WandB login for experiment tracking\n# ============================\n\n!wandb 'WANDB_API_KEY'\nimport wandb\nwandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:13:15.259000Z","iopub.execute_input":"2025-02-25T17:13:15.259301Z","iopub.status.idle":"2025-02-25T17:13:16.694285Z","shell.execute_reply.started":"2025-02-25T17:13:15.259275Z","shell.execute_reply":"2025-02-25T17:13:16.693524Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matickft13129\u001b[0m (\u001b[33matickft13129-green-university-of-bangladesh\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# ============================\n# Disable cache to prevent and Start training\n# ============================\n\n# Disable cache to prevent issues during training\npeft_model.config.use_cache = False\n\n# Start the training process\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:13:37.712471Z","iopub.execute_input":"2025-02-25T17:13:37.712819Z","iopub.status.idle":"2025-02-25T18:40:49.954045Z","shell.execute_reply.started":"2025-02-25T17:13:37.712780Z","shell.execute_reply":"2025-02-25T18:40:49.953199Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250225_171338-1rjhb7zs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/atickft13129-green-university-of-bangladesh/huggingface/runs/1rjhb7zs' target=\"_blank\">./Falcon-7b-Model-Finetuned-By-Mental-Health-Data</a></strong> to <a href='https://wandb.ai/atickft13129-green-university-of-bangladesh/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/atickft13129-green-university-of-bangladesh/huggingface' target=\"_blank\">https://wandb.ai/atickft13129-green-university-of-bangladesh/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/atickft13129-green-university-of-bangladesh/huggingface/runs/1rjhb7zs' target=\"_blank\">https://wandb.ai/atickft13129-green-university-of-bangladesh/huggingface/runs/1rjhb7zs</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 1:17:21, Epoch 3/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.659200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.668600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.690100</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.574500</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.449900</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.443900</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.391500</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.338700</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.274500</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.367300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=1.4858099102973938, metrics={'train_runtime': 5221.4497, 'train_samples_per_second': 0.123, 'train_steps_per_second': 0.002, 'total_flos': 2.384538496401408e+16, 'train_loss': 1.4858099102973938})"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# ============================\n# Push the fine-tuned model to Hugging Face Hub\n# ============================\n\ntrainer.push_to_hub()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T18:51:40.725808Z","iopub.execute_input":"2025-02-25T18:51:40.726156Z","iopub.status.idle":"2025-02-25T18:51:44.666889Z","shell.execute_reply.started":"2025-02-25T18:51:40.726134Z","shell.execute_reply":"2025-02-25T18:51:44.666130Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/FardinAbrar/Falcon-7b-model-Finetuned-By-Mental-Health-Data/commit/dbef2b83607bbfc3236636dbc5b9eff495947a34', commit_message='End of training', commit_description='', oid='dbef2b83607bbfc3236636dbc5b9eff495947a34', pr_url=None, repo_url=RepoUrl('https://huggingface.co/FardinAbrar/Falcon-7b-model-Finetuned-By-Mental-Health-Data', endpoint='https://huggingface.co', repo_type='model', repo_id='FardinAbrar/Falcon-7b-model-Finetuned-By-Mental-Health-Data'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# ============================\n# Loading the original model\n# ============================\n\nmodel_name = \"ybelkada/falcon-7b-sharded-bf16\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:02:08.579466Z","iopub.execute_input":"2025-02-25T19:02:08.579787Z","iopub.status.idle":"2025-02-25T19:03:56.108804Z","shell.execute_reply.started":"2025-02-25T19:02:08.579758Z","shell.execute_reply":"2025-02-25T19:03:56.107855Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/581 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b303d27d1ab43c99d426f4e4638c63f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807a44e8e5234080b562bf53542ff9b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2bc7ca1edfc4e1ca2d56f6bbac3fcdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/1.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd359171bf464bf39f8984d4e015fd45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8af84b3f018248bd883d883d21dadbeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"724f8fa9804a4a6894ca014ba5d25f36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e0f3cb5bb3148ba896401fdb397e3fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94eb0d0125743a8a4aecc9528ac0489"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df2a206e4ec2474ea5bfc2b82467a840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c7931e78a0c4073950a3a0158518822"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/921M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6caf6be264eb4211b2bec0b0983b060b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d417ec55ce1245e39c48d07561f299ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e48a6e37bcc74059adc411bf39e0f7ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d7c6b66e3ad4181b09f5641596327e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d347063e41dd4ee2b96eb2602d325680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc3c938f18e49e6954a72d3fcbdb183"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ============================\n# Load PEFT model from the Hub\n# ============================\n\n\nPEFT_MODEL = \"FardinAbrar/Falcon-7b-model-Finetuned-By-Mental-Health-Data\"\n\nconfig = PeftConfig.from_pretrained(PEFT_MODEL)\npeft_base_model = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    return_dict=True,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\npeft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)\n\n# Initialize PEFT tokenizer\npeft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\npeft_tokenizer.pad_token = peft_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:04:01.156315Z","iopub.execute_input":"2025-02-25T19:04:01.156615Z","iopub.status.idle":"2025-02-25T19:04:34.173700Z","shell.execute_reply.started":"2025-02-25T19:04:01.156592Z","shell.execute_reply":"2025-02-25T19:04:34.172946Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/778 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10b31cd4144f471995ce42026d1422de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31fc3afc737f4e9aa81b8b40aaa56fc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9257528a0c5d4f3e90e39cb52d2a650b"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ============================\n# Function to generate responses from both original model and PEFT model\n# ============================\n\ndef generate_answer(query):\n  system_prompt = \"\"\" \"\"\"\n\n  user_prompt = f\"\"\"<HUMAN>: {query}\n  <ASSISTANT>: \"\"\"\n\n  final_prompt = system_prompt + \"\\n\" + user_prompt\n\n  device = \"cuda:0\"\n  dashline = \"-\".join(\"\" for i in range(50))\n\n  # Generating response from the original model\n  encoding = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n  outputs = model.generate(input_ids=encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = tokenizer.eos_token_id, \\\n                                                                                                                     eos_token_id = tokenizer.eos_token_id, attention_mask = encoding.attention_mask, \\\n                                                                                                                     temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n  text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n  print(dashline)\n  print(f'ORIGINAL MODEL RESPONSE:\\n{text_output}')\n  print(dashline)\n\n  # Generating response from the PEFT model\n  peft_encoding = peft_tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n  peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = peft_tokenizer.eos_token_id, \\\n                                                                                                                     eos_token_id = peft_tokenizer.eos_token_id, attention_mask = peft_encoding.attention_mask, \\\n                                                                                                                     temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n  peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n\n  print(f'PEFT MODEL RESPONSE:\\n{peft_text_output}')\n  print(dashline)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:11:39.196919Z","iopub.execute_input":"2025-02-25T19:11:39.197291Z","iopub.status.idle":"2025-02-25T19:11:39.203650Z","shell.execute_reply.started":"2025-02-25T19:11:39.197260Z","shell.execute_reply":"2025-02-25T19:11:39.202717Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ============================\n# Testing the model's response generation\n# ============================\n\nquery = \"How can I prevent anxiety and depression?\"\ngenerate_answer(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:11:47.526581Z","iopub.execute_input":"2025-02-25T19:11:47.526885Z","iopub.status.idle":"2025-02-25T19:12:18.679891Z","shell.execute_reply.started":"2025-02-25T19:11:47.526860Z","shell.execute_reply":"2025-02-25T19:12:18.678951Z"}},"outputs":[{"name":"stdout","text":"-------------------------------------------------\nORIGINAL MODEL RESPONSE:\n \n<HUMAN>: How can I prevent anxiety and depression?\n  <ASSISTANT>: <HUMAN>, you can prevent anxiety and depression by taking a walk, exercising, meditating, and eating healthy foods.\n\nI'm not sure if this is the best way to do it, but it's a start.\n I'm not sure if I should use the <HUMAN> tag or not.\n @user2357112 I'm not sure if I should use the `` tag or not. I'm not sure if I should use the `` tag or not. I'm not sure if I should use the `` tag or not. I'm not sure if I should use the `` tag or not. I'm not sure if I should use the `` tag or not. I'm not sure if I should use the `` tag or not. I'm not sure if I should use the `` tag or not. I'm not sure if I should use the `` tag or not. I'm not sure if I should use the `` tag or not. I'm not sure if I should use the `` tag or not. I'm not sure if I should\n-------------------------------------------------\nPEFT MODEL RESPONSE:\n \n<HUMAN>: How can I prevent anxiety and depression?\n  <ASSISTANT>: 1. Practice self-care:\n\nTake care of your physical health by eating nutritious foods, exercising regularly, and getting enough sleep.\n\nPractice mindfulness techniques such as meditation or yoga to help reduce stress and improve mental well-being.\n\nSeek professional help if you feel overwhelmed by anxiety or depression.\n\n2. Engage in healthy activities:\n\nEngage in activities that bring you joy and make you feel fulfilled.\n\nSpend time with supportive friends and family members.\n\nParticipate in hobbies or interests that interest you.\n\n3. Seek support from others:\n\nReach out to loved ones for emotional support when you need it.\n\nJoin a support group or therapy sessions to connect with others who understand what you're going through.\n\n4. Adopt healthy coping strategies:\n\nIdentify unhealthy coping mechanisms and replace them with more productive ones.\n\nLearn how to manage difficult emotions without resorting to harmful behaviors.\n\nDevelop a plan for managing triggers and stressors that may lead to anxiety or depression.\n\n5. Practice self-compassion:\n\nRecognize that everyone experiences challenging times, and it's okay to seek help when needed.\n\nBe gentle with yourself and give yourself permission\n-------------------------------------------------\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================\n# Testing the model's response generation\n# ============================\n\nquery = \"How to take care of mental health?\"\ngenerate_answer(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:12:32.214641Z","iopub.execute_input":"2025-02-25T19:12:32.214975Z","iopub.status.idle":"2025-02-25T19:13:03.358424Z","shell.execute_reply.started":"2025-02-25T19:12:32.214945Z","shell.execute_reply":"2025-02-25T19:13:03.357569Z"}},"outputs":[{"name":"stdout","text":"-------------------------------------------------\nORIGINAL MODEL RESPONSE:\n \n<HUMAN>: How to take care of mental health?\n  <ASSISTANT>: <HUMAN>, you can take care of your mental health by doing the following things:\n<HUMAN>: 1. <ASSISTANT>: <HUMAN>, you can take care of your mental health by doing the following things:\n<HUMAN>: 2. <ASSISTANT>: <HUMAN>, you can take care of your mental health by doing the following things:\n<HUMAN>: 3. <ASSISTANT>: <HUMAN>, you can take care of your mental health by doing the following things:\n<HUMAN>: 4. <ASSISTANT>: <HUMAN>, you can take care of your mental health by doing the following things:\n<HUMAN>: 5. <ASSISTANT>: <HUMAN>, you can take care of your mental health by doing the following things:\n<HUMAN>: 6. <ASSISTANT>: <HUMAN>, you can take care of your mental health by doing the following things:\n<HUMAN>: 7. <ASSISTANT>: <HUMAN>, you can take care of\n-------------------------------------------------\nPEFT MODEL RESPONSE:\n \n<HUMAN>: How to take care of mental health?\n  <ASSISTANT>: 1. Take a break from work and spend time with loved ones.\n2. Engage in activities that bring you joy.\n3. Practice mindfulness and meditation.\n4. Seek professional help if needed.\n5. Maintain a healthy diet and exercise routine.\n6. Avoid negative thoughts and self-criticism.\n7. Get enough sleep and rest.\n8. Stay connected with friends and family.\n9. Find ways to cope with stress and anxiety.\n10. Learn coping skills and techniques.\n11. Be kind to yourself and practice self-compassion.\n12. Reach out for support when needed.\n13. Remember that mental health is a journey, not a destination.\n14. Celebrate small victories and progress.\n15. Don't be afraid to ask for help when needed.\n16. Be patient with yourself and give yourself grace.\n17. Accept that mental health is complex and unique to each individual.\n18. Understand that everyone experiences challenges and setbacks.\n19. Recognize your strengths and areas of growth.\n20. Practice gratitude and cultivate an attitude of gratitude.\n21. Adopt a positive mindset and focus on the positives.\n22. Develop resilience and learn how to bounce back from setbacks\n-------------------------------------------------\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================\n# Testing the model's response generation\n# ============================\n\nquery = \"What is the warning sign of depression?\"\ngenerate_answer(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:13:10.126173Z","iopub.execute_input":"2025-02-25T19:13:10.126473Z","iopub.status.idle":"2025-02-25T19:13:41.270057Z","shell.execute_reply.started":"2025-02-25T19:13:10.126450Z","shell.execute_reply":"2025-02-25T19:13:41.269369Z"}},"outputs":[{"name":"stdout","text":"-------------------------------------------------\nORIGINAL MODEL RESPONSE:\n \n<HUMAN>: What is the warning sign of depression?\n  <ASSISTANT>: <HUMAN> is depressed.\n\nI'm not sure if I'm doing something wrong or if I'm just not understanding how to use the <HUMAN> tag.\n You can use the <HUMAN> tag to represent a human, but it's not really meant for that. It's meant for representing a human in a conversation.\nFor example, if you want to say \"I'm depressed\", you would say \"I'm depressed\", and then you would say \"I'm depressed because...\".\nIf you want to say \"I'm depressed\", you would say \"I'm depressed\", and then you would say \"I'm depressed because...\".\nIf you want to say \"I'm depressed\", you would say \"I'm depressed\", and then you would say \"I'm depressed because...\".\nIf you want to say \"I'm depressed\", you would say \"I'm depressed\", and then you would say \"I'm depressed because...\".\nIf you want to say \"I'm depressed\", you would say \"I'm depressed\", and then you\n-------------------------------------------------\nPEFT MODEL RESPONSE:\n \n<HUMAN>: What is the warning sign of depression?\n  <ASSISTANT>: **Depression** is a mental health condition that can affect anyone, regardless of age, gender, or background. It is characterized by feelings of sadness, hopelessness, and/or low self-esteem.\n\nIf you or someone you know is experiencing these symptoms, it is important to seek professional help as soon as possible. There are many resources available, including support groups, therapy, and medication.\n\nRemember, it is not your fault, and there is hope for recovery. With proper treatment, you can overcome depression and lead a healthy, happy life.\n\n**If you or someone you know is struggling with depression, please reach out for help.**\n\nThere are many resources available, including support groups, therapy, and medication. Seeking help is the first step towards healing.\n\nRemember, you are not alone. Millions of people struggle with depression every day, and there is no shame in asking for help.\n\nWith proper treatment, you can overcome depression and lead a fulfilling life.\n\nPlease don't hesitate to reach out if you need support or guidance. We are here to help.\n\n**Remember, you are not alone.** I'm not sure if this is a good idea. I think it's better to\n-------------------------------------------------\n","output_type":"stream"}],"execution_count":13}]}